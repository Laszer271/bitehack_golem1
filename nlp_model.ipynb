{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "ubNhxIpo9FIP"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "from transformers import TFAutoModelForSequenceClassification\n",
    "from transformers import AutoTokenizer\n",
    "import numpy as np\n",
    "from scipy.special import softmax\n",
    "import csv\n",
    "import urllib.request\n",
    "import math\n",
    "import torch "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "aKrG7tAj9Gya"
   },
   "outputs": [],
   "source": [
    "# Preprocess text (username and link placeholders)\n",
    "def preprocess(text):\n",
    "    new_text = []\n",
    " \n",
    "    for t in text.split(\" \"):\n",
    "        t = '@user' if t.startswith('@') and len(t) > 1 else t\n",
    "        t = 'http' if t.startswith('http') else t\n",
    "        new_text.append(t)\n",
    "    return \" \".join(new_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "V0bl5XC39IYE"
   },
   "outputs": [],
   "source": [
    "# Tasks:\n",
    "# emoji, emotion, hate, irony, offensive, sentiment\n",
    "# stance/abortion, stance/atheism, stance/climate, stance/feminist, stance/hillary\n",
    "\n",
    "tasks = ['emotion', 'hate', 'irony', 'offensive', 'sentiment']\n",
    "model_paths = []\n",
    "\n",
    "for task in tasks:\n",
    "  model_path = f\"cardiffnlp/twitter-roberta-base-{task}\"\n",
    "  model_paths.append(model_path)\n",
    "    \n",
    "model_mapping = {task: model_paths[i] for i, task in enumerate(tasks)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['cardiffnlp/twitter-roberta-base-emotion', 'cardiffnlp/twitter-roberta-base-hate', 'cardiffnlp/twitter-roberta-base-irony', 'cardiffnlp/twitter-roberta-base-offensive', 'cardiffnlp/twitter-roberta-base-sentiment']\n"
     ]
    }
   ],
   "source": [
    "print(model_paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e2H1nhuQ-XnY"
   },
   "source": [
    "# Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "DTGHEM1c-W8s"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "rGdydRZ47jQr"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(147, 5)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('dataset_.csv')\n",
    "df = df[['Text', 'label', 'emoi', 'hashtags', 'Media URLs']]\n",
    "df = df.sample(frac=0.02)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "BJqzvjR8_RY4"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "y = df['label']\n",
    "df_train, df_test, y_train, y_test = train_test_split(df, y, test_size=0.2)\n",
    "df_train = df_train.reset_index(drop=True)\n",
    "df_test = df_test.reset_index(drop=True)\n",
    "X_train = df_train['Text']\n",
    "X_test = df_test['Text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>label</th>\n",
       "      <th>emoi</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>Media URLs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RT @Tejasswiholics_ : @BiggBoss @itsmetejasswi...</td>\n",
       "      <td>disappointed</td>\n",
       "      <td>[]</td>\n",
       "      <td>['TejasswiPrakash', 'BBQueenTejasswi']</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@cooperownbey i do actually check facebook mar...</td>\n",
       "      <td>angry</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Promises are broken \\r\\nTell me are you happy now</td>\n",
       "      <td>happy</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@MsKellyMHayes Happy birthday!</td>\n",
       "      <td>happy</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>RT @urstrulyMahesh : Congratulations team #HER...</td>\n",
       "      <td>happy</td>\n",
       "      <td>['üëè', 'üëè', 'üëè']</td>\n",
       "      <td>['HERO']</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text         label  \\\n",
       "0  RT @Tejasswiholics_ : @BiggBoss @itsmetejasswi...  disappointed   \n",
       "1  @cooperownbey i do actually check facebook mar...         angry   \n",
       "2  Promises are broken \\r\\nTell me are you happy now         happy   \n",
       "3                     @MsKellyMHayes Happy birthday!         happy   \n",
       "4  RT @urstrulyMahesh : Congratulations team #HER...         happy   \n",
       "\n",
       "              emoi                                hashtags Media URLs  \n",
       "0               []  ['TejasswiPrakash', 'BBQueenTejasswi']        NaN  \n",
       "1               []                                      []        NaN  \n",
       "2               []                                      []        NaN  \n",
       "3               []                                      []        NaN  \n",
       "4  ['üëè', 'üëè', 'üëè']                                ['HERO']        NaN  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "from tensorflow import keras\n",
    "hashtags = df_train['hashtags'].str.strip('[]').str.replace(\"'\", '').str.lower()\n",
    "\n",
    "emojis = df_train['emoi'].str.strip('[]').str.replace(\"'\", '').str.lower()\n",
    "emoticons_model = keras.models.load_model('emoticons.h5')\n",
    "\n",
    "images = df_train['Media URLs']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(117,) == (117,)\n"
     ]
    }
   ],
   "source": [
    "print(hashtags.shape, '==', X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "q5fytnTxkhqh"
   },
   "outputs": [],
   "source": [
    "def predict(model, tokenizer, preprocess, X, emb_max_size=512):\n",
    "  X = X.apply(preprocess)\n",
    "  encoded_input = tokenizer(X.to_list(), return_tensors='pt', padding=True)\n",
    "  encoded_input['input_ids'] = encoded_input['input_ids'][:, :emb_max_size]\n",
    "  encoded_input['attention_mask'] = encoded_input['attention_mask'][:, :emb_max_size]\n",
    "\n",
    "  output = model(**encoded_input)\n",
    "  return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextModel:\n",
    "    def __init__(self, model_path_mapping, batch_size=10):\n",
    "        self.batch_size = batch_size\n",
    "        self.model_path_mapping = model_path_mapping\n",
    "    \n",
    "    def predict(self, X, tasks_list, prefix='', verbose=1):\n",
    "        df = pd.DataFrame()\n",
    "        \n",
    "        for i, task in enumerate(tasks_list):\n",
    "            if verbose:\n",
    "                print(f'Step {i}/{len(tasks_list)}, Task: {task}')\n",
    "            model_path = self.model_path_mapping[task]\n",
    "            \n",
    "            tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "            model = AutoModelForSequenceClassification.from_pretrained(model_path)\n",
    "            tokenizer.save_pretrained(model_path)\n",
    "            model.save_pretrained(model_path) \n",
    "            \n",
    "            labels=[]\n",
    "            mapping_link = f\"https://raw.githubusercontent.com/cardiffnlp/tweeteval/main/datasets/{task}/mapping.txt\"\n",
    "            with urllib.request.urlopen(mapping_link) as f:\n",
    "                html = f.read().decode('utf-8').split(\"\\n\")\n",
    "                csvreader = csv.reader(html, delimiter='\\t')\n",
    "            labels = [row[1] for row in csvreader if len(row) > 1]\n",
    "            \n",
    "            outputs = []\n",
    "            n_batches = math.ceil(X.shape[0] / self.batch_size)\n",
    "            for i in range(n_batches):\n",
    "                if verbose > 1:\n",
    "                    print(i, '/', n_batches)\n",
    "                x = X[i*self.batch_size: (i+1)*self.batch_size]\n",
    "\n",
    "                out = predict(model, tokenizer, preprocess, x)\n",
    "                out['logits'] = out['logits'].cpu().detach()\n",
    "                outputs.append(out)\n",
    "                \n",
    "            output = {}\n",
    "            output['logits'] = torch.cat([out['logits'] for out in outputs], axis=0)\n",
    "            \n",
    "            scores = output['logits'].detach().numpy()\n",
    "            scores = softmax(scores, axis=1)\n",
    "            if verbose:\n",
    "                print('Output shape:', scores.shape)\n",
    "            \n",
    "            for i in range(scores.shape[1]):\n",
    "                label = labels[i]\n",
    "                df[prefix + label] = scores[:, i]\n",
    "        return df\n",
    "\n",
    "class HashtagModel(TextModel):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        \n",
    "    def predict(self, X, *args, **kwargs):\n",
    "        mask = X.str.len() == 0\n",
    "        df = super().predict(X, *args, **kwargs)\n",
    "        print(mask.shape, df.shape)\n",
    "        df.loc[mask, :] = 0.0\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X: (117,)\n",
      "Step 0/5, Task: emotion\n",
      "Output shape: (117, 4)\n",
      "X: (117,)\n",
      "Step 1/5, Task: hate\n",
      "Output shape: (117, 2)\n",
      "X: (117,)\n",
      "Step 2/5, Task: irony\n",
      "Output shape: (117, 2)\n",
      "X: (117,)\n",
      "Step 3/5, Task: offensive\n",
      "Output shape: (117, 2)\n",
      "X: (117,)\n",
      "Step 4/5, Task: sentiment\n",
      "Output shape: (117, 3)\n"
     ]
    }
   ],
   "source": [
    "text_model = TextModel(model_mapping)\n",
    "text_preds = text_model.predict(X_train, tasks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X: (117,)\n",
      "Step 0/5, Task: emotion\n",
      "Output shape: (117, 4)\n",
      "X: (117,)\n",
      "Step 1/5, Task: hate\n",
      "Output shape: (117, 2)\n",
      "X: (117,)\n",
      "Step 2/5, Task: irony\n",
      "Output shape: (117, 2)\n",
      "X: (117,)\n",
      "Step 3/5, Task: offensive\n",
      "Output shape: (117, 2)\n",
      "X: (117,)\n",
      "Step 4/5, Task: sentiment\n",
      "Output shape: (117, 3)\n",
      "(117,) (117, 13)\n"
     ]
    }
   ],
   "source": [
    "hashtag_model = HashtagModel(model_mapping)\n",
    "hashtag_preds = hashtag_model.predict(hashtags, tasks, prefix='hm_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import pickle\n",
    "import os\n",
    "import ast\n",
    "from joblib import dump, load\n",
    "\n",
    "class EmojiModel():\n",
    "    def __init__(self, emoji_data_path, optimizer='adam', loss='bce', metrics=None):\n",
    "        \n",
    "        with open(emoji_data_path, 'rb') as f:\n",
    "            self.emoji_data =  pickle.load(f)\n",
    "        self.keys = np.array(list(self.emoji_data.keys()))\n",
    "        \n",
    "        if metrics is None:\n",
    "            metrics = ['acc']\n",
    "\n",
    "        self.model = self.build_model(optimizer, loss, metrics)\n",
    "        self.encoder = None \n",
    "        self.categories = None\n",
    "        \n",
    "    def build_model(self, optimizer, loss, metrics):\n",
    "        model = keras.models.Sequential()\n",
    "        model.add(keras.layers.Input((len(self.keys),)))\n",
    "        model.add(keras.layers.Dropout(0.5))\n",
    "        model.add(keras.layers.Dense(256, activation='relu'))\n",
    "        model.add(keras.layers.Dropout(0.5))\n",
    "        model.add(keras.layers.Dense(256, activation='relu'))\n",
    "        model.add(keras.layers.Dense(4, activation='softmax'))\n",
    "\n",
    "        model.compile(optimizer=optimizer, metrics=metrics, loss=loss)\n",
    "        return model\n",
    "    \n",
    "    def preprocess(self, X):\n",
    "        X = X.str.split(', ')\n",
    "        mask = X.map(lambda d: len(d) > 2 or (len(d) == 1 and d[0] != '' and d[0] != ' '))\n",
    "        encoded = np.zeros((len(X), self.keys.shape[0]))\n",
    "        \n",
    "        for i, x in enumerate(X):\n",
    "            if mask[i]:\n",
    "                idx = self._find_indices(x)\n",
    "                if len(idx) > 0:\n",
    "                    encoded[i, idx] = 1\n",
    "            \n",
    "        return encoded\n",
    "    \n",
    "    def fit(self, X, y, *args, validation_data=None, **kwargs):\n",
    "        mask = X.str.split(', ').map(lambda d: len(d) > 2 or (len(d) == 1 and d[0] != '' and d[0] != ' '))\n",
    "        X = self.preprocess(X)\n",
    "        y = self.encode_y(y)\n",
    "        \n",
    "        X = X[mask]\n",
    "        y = y[mask]\n",
    "        \n",
    "        if validation_data is not None:\n",
    "            validation_data = (self.preprocess(validation_data[0]), self.encode_y(validation_data[1]))\n",
    "        \n",
    "        return self.model.fit(X, y, *args, validation_data=validation_data, **kwargs)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        mask = X.str.split(', ').map(lambda d: len(d) > 2 or (len(d) == 1 and d[0] != '' and d[0] != ' '))\n",
    "        X = self.preprocess(X)\n",
    "        preds = self.model.predict(X)\n",
    "        preds[~mask] = 0\n",
    "        return preds\n",
    "    \n",
    "    def encode_y(self, y):\n",
    "        if self.encoder is None:\n",
    "            self.classes = np.unique(y)\n",
    "            print(self.classes)\n",
    "            self.encoder = OneHotEncoder(handle_unknown='ignore')#, categories=self.classes)\n",
    "            self.encoder.fit(pd.DataFrame(y))\n",
    "            \n",
    "            \n",
    "        return self.encoder.transform(pd.DataFrame(y)).toarray()\n",
    "    \n",
    "    def evaluate(self, X, y):\n",
    "        y = self.encode_y(y)\n",
    "        preds = self.predict(X)\n",
    "        \n",
    "        d = {\n",
    "            'MSE': ((y - preds) ** 2).mean(),\n",
    "            'MAE': np.abs((y - preds)).mean(),\n",
    "            'accuracy': sum(np.argmax(preds, axis=1) == np.argmax(y, axis=1)) / len(y)\n",
    "        }\n",
    "        \n",
    "        return d\n",
    "    \n",
    "    def _find_indices(self, values):\n",
    "        columns = self.keys.flatten()\n",
    "        inds = np.array([np.where(columns == searchval)[0] for searchval in values if searchval in self.keys]).flatten()\n",
    "        #print('values:', values)\n",
    "        #print('columns:', columns.shape)\n",
    "        #print('inds:', inds)\n",
    "        return inds\n",
    "    \n",
    "    def save_model(self, model_path, encoder_path):\n",
    "        dump(self.encoder, encoder_path) # save the model\n",
    "        self.model.save(model_path)\n",
    "        \n",
    "    \n",
    "    def load_model(self, model_path, encoder_path):\n",
    "        self.encoder = load(encoder_path) # load and reuse the model\n",
    "        self.model = keras.models.load_model(model_path)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = emojis.str.split(', ').map(lambda d: len(d) > 2 or (len(d) == 1 and d[0] != '' and d[0] != ' '))\n",
    "#emojis[mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['angry' 'disappointed' 'happy' 'sad']\n",
      "Epoch 1/10\n",
      "2/2 [==============================] - 1s 180ms/step - loss: 0.6951 - acc: 0.1724 - val_loss: 0.6892 - val_acc: 0.2000\n",
      "Epoch 2/10\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 0.6853 - acc: 0.4138 - val_loss: 0.6846 - val_acc: 0.2000\n",
      "Epoch 3/10\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.6756 - acc: 0.3448 - val_loss: 0.6792 - val_acc: 0.2000\n",
      "Epoch 4/10\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.6685 - acc: 0.4138 - val_loss: 0.6733 - val_acc: 0.2333\n",
      "Epoch 5/10\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 0.6560 - acc: 0.4828 - val_loss: 0.6666 - val_acc: 0.2333\n",
      "Epoch 6/10\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.6435 - acc: 0.4483 - val_loss: 0.6592 - val_acc: 0.2333\n",
      "Epoch 7/10\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.6364 - acc: 0.4483 - val_loss: 0.6510 - val_acc: 0.2333\n",
      "Epoch 8/10\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.6197 - acc: 0.5517 - val_loss: 0.6419 - val_acc: 0.2333\n",
      "Epoch 9/10\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 0.6162 - acc: 0.5172 - val_loss: 0.6321 - val_acc: 0.2000\n",
      "Epoch 10/10\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 0.6038 - acc: 0.4828 - val_loss: 0.6222 - val_acc: 0.2000\n"
     ]
    }
   ],
   "source": [
    "emoji_model = EmojiModel('emoji_data/UNICODE_EMOJI_ALIAS.pkl')\n",
    "emojis_test = df_test['emoi'].str.strip('[]').str.replace(\"'\", '').str.lower()\n",
    "emoji_model.fit(emojis, y=df_train['label'], batch_size=16, epochs=10, validation_data=(emojis_test, df_test['label']))\n",
    "\n",
    "emoji_preds = emoji_model.predict(emojis)\n",
    "\n",
    "score = emoji_model.evaluate(emojis_test, df_test['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "emoji_model.save_model('emoji_model.h5', 'encoder.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'MSE': 0.23224514716775688,\n",
       " 'MAE': 0.2820870446662108,\n",
       " 'accuracy': 0.16666666666666666}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emoji_model = EmojiModel('emoji_data/UNICODE_EMOJI_ALIAS.pkl')\n",
    "emoji_model.load_model('emoji_model.h5', 'encoder.joblib')\n",
    "emoji_model.evaluate(emojis_test, df_test['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'MSE': 0.23224514716775688,\n",
       " 'MAE': 0.2820870446662108,\n",
       " 'accuracy': 0.16666666666666666}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import fer\n",
    "import requests\n",
    "\n",
    "class ImageModel():\n",
    "    def __init__(self, prefix='im_'):\n",
    "        self.template = {'angry': 0.0, 'disgust': 0.0, 'fear': 0.0, 'happy': 0.0, 'sad': 0.0, 'surprise': 0.0, 'neutral': 0.0}\n",
    "        self.detector = fer.FER()\n",
    "        self.prefix = prefix\n",
    "    \n",
    "    def predict(self, X):\n",
    "        preds = []\n",
    "        for i, url in enumerate(X):\n",
    "            print(f'{i}: {url}')\n",
    "            if pd.isna(url):\n",
    "                preds.append(self.template.copy())\n",
    "                continue\n",
    "            img_data = requests.get(url).content\n",
    "            with open('./sample/images/temp.png', 'wb') as f:\n",
    "                f.write(img_data)\n",
    "            \n",
    "            bad_net_img = cv2.imread('./sample/images/temp.png')\n",
    "            try:\n",
    "                pred = self.detector.detect_emotions(bad_net_img)\n",
    "            except:\n",
    "                pred = []\n",
    "            if len(pred) > 0:\n",
    "                pred = [p['emotions'] for p in pred]\n",
    "                final_pred = self.template.copy()\n",
    "                for j, p in enumerate(pred, 1):\n",
    "                    for key in p.keys():\n",
    "                        final_pred[key] += p[key]\n",
    "                for key in p.keys():\n",
    "                    final_pred[key] /= j\n",
    "                        \n",
    "                print(final_pred)\n",
    "                preds.append(final_pred)\n",
    "            else:\n",
    "                preds.append(self.template.copy())\n",
    "                \n",
    "        df = pd.DataFrame(preds)\n",
    "        df.columns = [self.prefix + col for col in df.columns]\n",
    "        return df\n",
    "                \n",
    "                \n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_model = ImageModel()\n",
    "image_preds = image_model.predict(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.loc[38, 'Text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.loc[60, 'Text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(text_preds.shape, hashtag_preds.shape, emoji_preds.shape, image_preds.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(text_preds), type(hashtag_preds), type(emoji_preds), type(image_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([text_preds, hashtag_preds, pd.DataFrame(emoji_preds), image_preds], axis=1)\n",
    "df.to_excel('dataset_preds_all_models.xlsx')\n",
    "df.to_csv('dataset_preds_all_models.csv', index=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "Model_test.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
